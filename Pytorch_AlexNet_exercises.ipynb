{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucianoselimaj/MLDL_Labs/blob/main/Pytorch_AlexNet_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch AlexNet Exercises\n",
        "\n",
        "Welcome to the PyTorch AlexNet exercise template notebook.\n",
        "\n",
        "There are several questions in this notebook and it's your goal to answer them by writing Python and PyTorch code.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G-lX9oW_OghS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we'll cover:\n",
        "1. Define full AlexNet architecture\n",
        "2. Implement training & validation loops\n",
        "3. Insert result tracking and reporting\n",
        "4. (Optional) Add model saving and decay experiments\n"
      ],
      "metadata": {
        "id": "HKRr7-KSfC06"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mgA4k7F9OWxM",
        "outputId": "ae8dab31-11cf-4eff-8ee9-459dab64cb24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiny ImageNet already downloaded.\n",
            "üîß Reorganizing validation folder...\n",
            "‚úÖ Validation folder reorganized.\n",
            "\n",
            "üîÅ Training AlexNet | LR=0.1, Batch Size=16\n",
            "Epoch [1/10] ‚Üí Train Loss: 5.3136 | Train Acc: 0.43%\n",
            "              Val Loss: 5.3112 | Val Acc: 0.50%\n",
            "Epoch [2/10] ‚Üí Train Loss: 5.3133 | Train Acc: 0.46%\n",
            "              Val Loss: 5.3117 | Val Acc: 0.50%\n",
            "Epoch [3/10] ‚Üí Train Loss: 5.3137 | Train Acc: 0.50%\n",
            "              Val Loss: 5.3126 | Val Acc: 0.50%\n",
            "Epoch [4/10] ‚Üí Train Loss: 5.3134 | Train Acc: 0.49%\n",
            "              Val Loss: 5.3109 | Val Acc: 0.50%\n",
            "Epoch [5/10] ‚Üí Train Loss: 5.3139 | Train Acc: 0.48%\n",
            "              Val Loss: 5.3076 | Val Acc: 0.50%\n",
            "Epoch [6/10] ‚Üí Train Loss: 5.3130 | Train Acc: 0.50%\n",
            "              Val Loss: 5.3129 | Val Acc: 0.50%\n",
            "Epoch [7/10] ‚Üí Train Loss: 5.3134 | Train Acc: 0.46%\n",
            "              Val Loss: 5.3124 | Val Acc: 0.50%\n",
            "Epoch [8/10] ‚Üí Train Loss: 5.3128 | Train Acc: 0.48%\n",
            "              Val Loss: 5.3159 | Val Acc: 0.50%\n",
            "Epoch [9/10] ‚Üí Train Loss: 5.3132 | Train Acc: 0.49%\n",
            "              Val Loss: 5.3114 | Val Acc: 0.50%\n",
            "Epoch [10/10] ‚Üí Train Loss: 5.3134 | Train Acc: 0.49%\n",
            "              Val Loss: 5.3139 | Val Acc: 0.50%\n",
            "‚úÖ Saved: alexnet_lr0.1_bs16.pth\n",
            "\n",
            "üîÅ Training AlexNet | LR=0.1, Batch Size=32\n",
            "Epoch [1/10] ‚Üí Train Loss: 5.3063 | Train Acc: 0.51%\n",
            "              Val Loss: 5.3049 | Val Acc: 0.50%\n",
            "Epoch [2/10] ‚Üí Train Loss: 5.3060 | Train Acc: 0.49%\n",
            "              Val Loss: 5.3040 | Val Acc: 0.50%\n",
            "Epoch [3/10] ‚Üí Train Loss: 5.3062 | Train Acc: 0.47%\n",
            "              Val Loss: 5.3055 | Val Acc: 0.50%\n",
            "Epoch [4/10] ‚Üí Train Loss: 5.3066 | Train Acc: 0.48%\n",
            "              Val Loss: 5.3039 | Val Acc: 0.50%\n",
            "Epoch [5/10] ‚Üí Train Loss: 5.3065 | Train Acc: 0.45%\n",
            "              Val Loss: 5.3041 | Val Acc: 0.50%\n",
            "Epoch [6/10] ‚Üí Train Loss: 5.3064 | Train Acc: 0.48%\n",
            "              Val Loss: 5.3042 | Val Acc: 0.50%\n",
            "Epoch [7/10] ‚Üí Train Loss: 5.3062 | Train Acc: 0.49%\n",
            "              Val Loss: 5.3040 | Val Acc: 0.50%\n",
            "Epoch [8/10] ‚Üí Train Loss: 5.3062 | Train Acc: 0.46%\n",
            "              Val Loss: 5.3056 | Val Acc: 0.50%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-26c2cdceea42>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Move input images and labels to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Clear previous gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#core lib for ML\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim #optimizers\n",
        "# provides standard datasets, transformers for data preprocessing\n",
        "from torchvision import datasets, transforms, models\n",
        "# data batch loading\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Define dataset directory\n",
        "dataset_dir = './tiny-imagenet-200'\n",
        "zip_path = './tiny-imagenet-200.zip'\n",
        "url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
        "\n",
        "\n",
        "\n",
        "#Download the dataset if it doesn't exist\n",
        "if not os.path.exists(dataset_dir):\n",
        "    print(\"Downloading Tiny ImageNet dataset...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "    print(\"Dataset ready!\")\n",
        "else:\n",
        "    print(\"Tiny ImageNet already downloaded.\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Step 2: Reorganize Validation Folder (only first time)\n",
        "# --------------------------------------------------\n",
        "\n",
        "val_dir = os.path.join(dataset_dir, 'val')\n",
        "val_images_dir = os.path.join(val_dir, 'images')\n",
        "val_annotations_file = os.path.join(val_dir, 'val_annotations.txt')\n",
        "\n",
        "# Only reorganize if not already done\n",
        "if not os.path.exists(os.path.join(val_dir, 'n01443537')):\n",
        "    print(\"üîß Reorganizing validation folder...\")\n",
        "    val_annotations = defaultdict(list)\n",
        "\n",
        "    with open(val_annotations_file, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            parts = line.strip().split('\\t')\n",
        "            filename, class_name = parts[0], parts[1]\n",
        "            val_annotations[class_name].append(filename)\n",
        "\n",
        "    for class_name, filenames in val_annotations.items():\n",
        "        class_dir = os.path.join(val_dir, class_name, 'images')\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "        for fname in filenames:\n",
        "            src = os.path.join(val_images_dir, fname)\n",
        "            dst = os.path.join(class_dir, fname)\n",
        "            shutil.move(src, dst)\n",
        "\n",
        "    shutil.rmtree(val_images_dir)\n",
        "    print(\"‚úÖ Validation folder reorganized.\")\n",
        "\n",
        "\n",
        "\n",
        "# Define a custom AlexNet architecture using PyTorch\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=200):  # Tiny ImageNet has 200 classes\n",
        "        super(AlexNet, self).__init__()  # Initialize the parent nn.Module class\n",
        "\n",
        "        # Define the convolutional feature extractor part of AlexNet\n",
        "        self.features = nn.Sequential(\n",
        "            # First convolutional layer: input has 3 channels (RGB), output 64 feature maps\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),  # Apply ReLU activation function\n",
        "\n",
        "            # Max pooling layer to reduce spatial dimensions (size and complexity)\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "            # Second convolutional layer: 64 input channels, 192 output filters\n",
        "            nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),  # Apply ReLU activation\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # Again, reduce dimensions\n",
        "\n",
        "            # Third convolutional layer: increases depth to 384 feature maps\n",
        "            nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Fourth convolutional layer: reduces to 256 feature maps\n",
        "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Fifth convolutional layer: maintains 256 filters\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Final pooling to reduce to a small spatial feature map (typically 6x6)\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        # Define the fully connected layers (classifier part)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),  # Dropout: randomly deactivate neurons (for regularization)\n",
        "\n",
        "            # First fully connected layer: input = flattened feature map (256√ó6√ó6), output = 4096 neurons\n",
        "            nn.Linear(in_features=256 * 6 * 6, out_features=4096),\n",
        "            nn.ReLU(inplace=True),  # ReLU activation\n",
        "\n",
        "            nn.Dropout(),  # Another dropout layer for regularization\n",
        "\n",
        "            # Second fully connected layer: 4096 ‚Üí 4096 neurons\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Final fully connected layer: maps to 200 output classes (TinyImageNet)\n",
        "            nn.Linear(in_features=4096, out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    # Define the forward pass: how data moves through the network\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)              # Pass input through convolutional layers\n",
        "        x = torch.flatten(x, 1)           # Flatten output (preserve batch dim)\n",
        "        x = self.classifier(x)            # Pass through fully connected layers\n",
        "        return x                          # Output: class scores (logits)\n",
        "\n",
        "\n",
        "# List of learning rates to test in your experiments\n",
        "learning_rates = [0.1, 0.001, 0.0001]\n",
        "# List of batch sizes to test\n",
        "batch_sizes = [16, 32, 64]\n",
        "# Number of epochs to train the model for each combination\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "# Define the image transformations (resizing, normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize all images to 224x224 (required by AlexNet)\n",
        "    transforms.ToTensor(),  # Convert PIL images to PyTorch tensors\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize using ImageNet means\n",
        "                         std=[0.229, 0.224, 0.225])   # And standard deviations\n",
        "])\n",
        "\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "val_dir = os.path.join(dataset_dir, 'val')\n",
        "# Load the training dataset (organize images in class folders)\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
        "# Load the validation dataset\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n",
        "\n",
        "\n",
        "# Loop over all combinations of learning rate and batch size\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        # Print current configuration being trained\n",
        "        print(f\"\\nüîÅ Training AlexNet | LR={lr}, Batch Size={batch_size}\")\n",
        "\n",
        "        # Create DataLoader for training set\n",
        "        # batch_size controls how many images are processed per step\n",
        "        # shuffle=True to randomly mix training data every epoch\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Create DataLoader for validation set\n",
        "        # shuffle=False since we just need performance stats\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Initialize a new instance of AlexNet\n",
        "        # .cuda() moves the model to the GPU for faster training\n",
        "        net = AlexNet(num_classes=200).cuda()\n",
        "\n",
        "        # Define the loss function: CrossEntropyLoss is used for multi-class classification\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Define the optimizer: SGD with momentum and weight decay\n",
        "        # lr: learning rate (from loop), momentum improves convergence speed\n",
        "        # weight_decay adds L2 regularization to prevent overfitting\n",
        "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "        # Main training loop: run for num_epochs iterations\n",
        "        for epoch in range(num_epochs):\n",
        "            net.train()  # Set the model to training mode\n",
        "            running_loss = 0.0  # Accumulate total loss per epoch\n",
        "            correct = 0         # Count correct predictions\n",
        "            total = 0           # Count total predictions\n",
        "\n",
        "            # Iterate over all training batches\n",
        "            for inputs, targets in train_loader:\n",
        "                # Move input images and labels to GPU\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                optimizer.zero_grad()      # Clear previous gradients\n",
        "                outputs = net(inputs)      # Forward pass through the model\n",
        "                loss = criterion(outputs, targets)  # Compute loss\n",
        "                loss.backward()            # Backpropagation (compute gradients)\n",
        "                optimizer.step()           # Update model weights\n",
        "\n",
        "                running_loss += loss.item()  # Add current batch loss to total\n",
        "\n",
        "                # Get predictions by choosing class with highest logit\n",
        "                _, predicted = outputs.max(1)\n",
        "\n",
        "                # Update total number of examples and correct predictions\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Compute training accuracy after all batches\n",
        "            train_acc = 100. * correct / total\n",
        "\n",
        "            # Print training loss and accuracy for this epoch\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] ‚Üí \"\n",
        "                  f\"Train Loss: {running_loss/len(train_loader):.4f} | \"\n",
        "                  f\"Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "            # ----------------- Validation Phase -----------------\n",
        "            net.eval()  # Set model to evaluation mode (disables dropout, batchnorm update)\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():  # No gradient tracking during validation\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                    outputs = net(inputs)              # Forward pass\n",
        "                    loss = criterion(outputs, targets) # Compute loss\n",
        "                    val_loss += loss.item()            # Accumulate loss\n",
        "\n",
        "                    # Get predicted classes\n",
        "                    _, predicted = outputs.max(1)\n",
        "\n",
        "                    # Update total and correct predictions\n",
        "                    val_total += targets.size(0)\n",
        "                    val_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            val_acc = 100. * val_correct / val_total\n",
        "\n",
        "            # Print validation loss and accuracy\n",
        "            print(f\"              Val Loss: {val_loss/len(val_loader):.4f} | \"\n",
        "                  f\"Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # After all epochs, save the trained model to a file\n",
        "        model_name = f\"alexnet_lr{lr}_bs{batch_size}.pth\"\n",
        "        torch.save(net.state_dict(), model_name)\n",
        "        print(f\"‚úÖ Saved: {model_name}\")\n",
        "\n"
      ]
    }
  ]
}