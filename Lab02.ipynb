{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucianoselimaj/MLDL_Labs/blob/main/Lab02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 02: Training a Custom Model\n"
      ],
      "metadata": {
        "id": "0JP9Qqn5fq2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective of this lab**: training a small custom model on the Tiny-ImageNet dataset."
      ],
      "metadata": {
        "id": "IA2wrFjFna7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation"
      ],
      "metadata": {
        "id": "YF_SzW86f8kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip tiny-imagenet-200.zip -d tiny-imagenet"
      ],
      "metadata": {
        "id": "ecXsQI0_f6pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to adjust the format of the val split of the dataset to be used with ImageFolder."
      ],
      "metadata": {
        "id": "Vei0UbVTkkPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os # built-in module for interacting with the operationg system\n",
        "import shutil #utility module to perform high-level file operations (e.g copying or removing files or directories)\n",
        "\n",
        "# Opens the file\n",
        "with open('tiny-imagenet/tiny-imagenet-200/val/val_annotations.txt') as f:\n",
        "  # for each line in the file\n",
        "    for line in f:\n",
        "      #splits each line using the tab --> filename, class label, *_ a catch-all for the rest of the content\n",
        "        fn, cls, *_ = line.split('\\t')\n",
        "        #creates a new directory for each class inside the val dir, excluding the throwing if the directory already exists\n",
        "        os.makedirs(f'tiny-imagenet/tiny-imagenet-200/val/{cls}', exist_ok=True)\n",
        "        # copies the image file\n",
        "        shutil.copyfile(f'tiny-imagenet/tiny-imagenet-200/val/images/{fn}', f'tiny-imagenet/tiny-imagenet-200/val/{cls}/{fn}')\n",
        "# once all images are copied to their new class folder, this deletes the original flat\n",
        "shutil.rmtree('tiny-imagenet/tiny-imagenet-200/val/images')"
      ],
      "metadata": {
        "id": "HuUvU_Gug7Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After downloading the dataset, which is stored as : val/images/\n",
        "\n",
        "We create a new set of subfolders for each class label:  <br>\n",
        "val/n01443537/val_1.JPEG <br>\n",
        "val/n01443537/val_2.JPEG <br>\n",
        "... <br>\n",
        "val/n01440764/val_3.JPEG <br>\n",
        "... <br>\n"
      ],
      "metadata": {
        "id": "cFknPOPsumqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following step we load the dataset applying transformations required"
      ],
      "metadata": {
        "id": "j6K4El2Sv6kg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt1X5euKfoEd"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as T\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.Resize((224, 224)),  # Resize to fit the input dimensions of the network\n",
        "    T.ToTensor(), # scales pixel values from 0–255 → 0–1 (float32), with shape [C,H,W]\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# root/{classX}/x001.jpg\n",
        "tiny_imagenet_dataset_train = ImageFolder(root='tiny-imagenet/tiny-imagenet-200/train', transform=transform)\n",
        "tiny_imagenet_dataset_val = ImageFolder(root='tiny-imagenet/tiny-imagenet-200/val', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of train dataset: {len(tiny_imagenet_dataset_train)}\")\n",
        "print(f\"Length of val dataset: {len(tiny_imagenet_dataset_val)}\")\n",
        "\n",
        "# The following code also checks the number of samples per class\n",
        "# from collections import Counter\n",
        "\n",
        "# class_counts = Counter([target for _, target in tiny_imagenet_dataset_val])\n",
        "# for class_label, count in class_counts.items():\n",
        "#     print(f\"Class {class_label}: {count} entries\")\n"
      ],
      "metadata": {
        "id": "AtIrYRaxg6pw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c1e1150-8e4c-4443-872f-301f6f7797bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset: 100000\n",
            "Length of val dataset: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "train_loader = torch.utils.data.DataLoader(tiny_imagenet_dataset_train, batch_size=32, shuffle=True, num_workers=8) # use 8 CPU workers (or process) to load the images in the backgorund while the GPU is training\n",
        "val_loader = torch.utils.data.DataLoader(tiny_imagenet_dataset_val, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "ExF2yRw9mT8G",
        "outputId": "b4fd6416-0ceb-4442-f716-c9234cc5d741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom model definition"
      ],
      "metadata": {
        "id": "m0AIe8afloCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNNs are made of 3 layers: <br>\n",
        "\n",
        "\n",
        "1.   Convolutional Layer (nn.Conv2d)\n",
        "This layer slides small filters across the image to detect features like edges or corners\n",
        "2.   Activation function\n",
        "3.   Fully connected layer (nn.Linear)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ge6hrow0q5s5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Define the custom neural network\n",
        "class CustomNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomNet, self).__init__()\n",
        "        # Define layers of the neural network\n",
        "        # input has 3 color channels RGB, with 64 different filters, each filter is 3x3 size, padding takes the same size --> input is 224x224, output is also 224x224\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, stride=1) #Output: 64 channels → it creates 64 different feature maps (filters)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # takes 64 channels from conv1 and applies 128 new filters\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        # we don't need every tiny detail from the image so we use Pooling to shrink the image\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) #reduces the image size by half 224 → 112\n",
        "\n",
        "        self.fc1 = nn.Linear(256, 200) # 200 is the number of classes in TinyImageNet\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define forward pass\n",
        "        # B x 3 x 224 x 224 --> inpus shape, Batch,Channel,height,weight\n",
        "        x = self.conv1(x).relu()     # [B, 64, 224, 224]\n",
        "        x = self.pool(x)             # [B, 64, 112, 112]\n",
        "\n",
        "        x = self.conv2(x).relu()     # [B, 128, 112, 112]\n",
        "        x = self.pool(x)             # [B, 128, 56, 56]\n",
        "\n",
        "        x = self.conv3(x).relu()     # [B, 256, 56, 56]\n",
        "        x = self.pool(x)             # [B, 256, 28, 28]\n",
        "\n",
        "        # Keep pooling until we get to [B, 256, 1, 1]\n",
        "        x = self.pool(x)             # [B, 256, 14, 14]\n",
        "        x = self.pool(x)             # [B, 256, 7, 7]\n",
        "        x = self.pool(x)             # [B, 256, 3, 3]\n",
        "        x = self.pool(x)             # [B, 256, 1, 1]\n",
        "\n",
        "        x = x.view(x.size(0), -1)    # flatten: [B, 256]\n",
        "        x = self.fc1(x)              # [B, 200]\n",
        "        return x"
      ],
      "metadata": {
        "id": "vpdGGfa8lDde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is missing in the code:\n",
        "\n",
        "\n",
        "1.   Zero the gradients\n",
        "2.   Forward pass\n",
        "3.   Compute the Loss\n",
        "4.   Backward pass (Backpropagation)\n",
        "5.   Optiomizer step (update weights)"
      ],
      "metadata": {
        "id": "KIi6VIedZQ-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, model, train_loader, criterion, optimizer):\n",
        "    #set the model to training mode\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        #Zero the gradients (accumulated by default)\n",
        "        optimizer.zero_grad() # clears the old gradients\n",
        "        #forward\n",
        "        outputs = model(inputs)\n",
        "        #Loss function\n",
        "        loss = criterion(outputs, targets)\n",
        "        #Backpropagation\n",
        "        loss.backward()\n",
        "        #optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100. * correct / total\n",
        "    print(f'Train Epoch: {epoch} Loss: {train_loss:.6f} Acc: {train_accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "zM0eatFllREw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation: <br>\n",
        "In this step:\n",
        "\n",
        "\n",
        "*   We don't do Backpropagation\n",
        "*   We don't updates weights\n",
        "*   We only evaluate how good the model is"
      ],
      "metadata": {
        "id": "6yS0PVs80VBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation loop\n",
        "def validate(model, val_loader, criterion):\n",
        "  #Set the model to validation mode\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            #Forward pass\n",
        "            outputs = model(inputs)\n",
        "            #Loss function\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = 100. * correct / total\n",
        "\n",
        "    print(f'Validation Loss: {val_loss:.6f} Acc: {val_accuracy:.2f}%')\n",
        "    return val_accuracy"
      ],
      "metadata": {
        "id": "LUicYRJamITD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting everything together"
      ],
      "metadata": {
        "id": "Qhr8xxEUmmGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomNet().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "best_acc = 0\n",
        "\n",
        "# Run the training process for {num_epochs} epochs\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(epoch, model, train_loader, criterion, optimizer)\n",
        "\n",
        "    # At the end of each training iteration, perform a validation step\n",
        "    val_accuracy = validate(model, val_loader, criterion)\n",
        "\n",
        "    # Best validation accuracy\n",
        "    best_acc = max(best_acc, val_accuracy)\n",
        "\n",
        "\n",
        "print(f'Best validation accuracy: {best_acc:.2f}%')\n"
      ],
      "metadata": {
        "id": "6lO-VJtZml6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}