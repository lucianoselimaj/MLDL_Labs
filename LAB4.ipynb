{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucianoselimaj/MLDL_Labs/blob/main/LAB4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 4: Transfer Learning and Visualizations\n",
        "\n",
        "\n",
        "> **Note:** There may be more than one solution to each of the exercises, don't worry too much about the *exact* right answer. Try to write some code that works first and then improve it if you can.\n",
        "\n"
      ],
      "metadata": {
        "id": "zNqPNlYylluR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install missing dependencies and import libraries"
      ],
      "metadata": {
        "id": "GjaLKzhZ1hNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install torchinfo (for model summary), torchmetrics (for metrics), and wandb (for logging)\n",
        "!pip install -q torchinfo torchmetrics wandb"
      ],
      "metadata": {
        "id": "-ZGi2gTq1uiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch core and neural network modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Import torchvision for pre-trained models and image utilities\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# Additional utilities\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import wandb  # Weights & Biases: for tracking experiments\n",
        "\n",
        "# Import for data loading and model summary\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "\n",
        "# For timing training\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# For accuracy and confusion matrix metrics\n",
        "from torchmetrics.functional import accuracy\n",
        "from torchmetrics.functional.classification import multiclass_confusion_matrix"
      ],
      "metadata": {
        "id": "Xm07jqAX1vhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically use GPU if available, otherwise fallback to CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "jsz6vZjq1wid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: plot a confusion matrix of the predictions on the test set"
      ],
      "metadata": {
        "id": "nwmoMhW8IqSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get data"
      ],
      "metadata": {
        "id": "nrzg3TaSKLAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the pizza_steak_sushi dataset (from GitHub)\n",
        "!wget https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\n",
        "# Create data directory (if it doesn't already exist)\n",
        "!mkdir -p data\n",
        "# Unzip the dataset into the data/ folder (only unzip if not done already)\n",
        "!unzip -n pizza_steak_sushi.zip -d data/pizza_steak_sushi"
      ],
      "metadata": {
        "id": "Sbat0jBN10pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data"
      ],
      "metadata": {
        "id": "PGaMWWaoKQlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transforms pipeline\n",
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "])"
      ],
      "metadata": {
        "id": "VNIQNEQVKVXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# LOAD DATASETS\n",
        "# -------------------------------\n",
        "# Set path to dataset\n",
        "data_path = \"data/pizza_steak_sushi\"\n",
        "\n",
        "# Load training and testing datasets using ImageFolder (expects subfolders for each class)\n",
        "train_dset = datasets.ImageFolder(f\"{data_path}/train\", transform=simple_transform)\n",
        "test_dset = datasets.ImageFolder(f\"{data_path}/test\", transform=simple_transform)\n",
        "\n",
        "# Save class names (pizza, steak, sushi)\n",
        "class_names = train_dset.classes\n",
        "\n",
        "# -------------------------------\n",
        "# CREATE DATA LOADERS\n",
        "# -------------------------------\n",
        "# Prepare data for training (batching and shuffling)\n",
        "train_dataloader = DataLoader(train_dset, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_dataloader = DataLoader(test_dset, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "Njd5lHTcKW23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get and prepare a pretrained model"
      ],
      "metadata": {
        "id": "Ciw2DiRHKaSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the model with pretrained weights and send it to the target device\n",
        "model_0 = torchvision.models.efficientnet_b0(pretrained=True).to(device)\n",
        "#model_0 # uncomment to output (it's very long)"
      ],
      "metadata": {
        "id": "snUuRXd8Kdk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
        "for param in model_0.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "IbRhGvy_KeVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Get the length of class_names (one output unit for each class)\n",
        "output_shape = len(class_names)\n",
        "\n",
        "# Recreate the classifier layer and seed it to the target device\n",
        "model_0.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(in_features=1280,\n",
        "                    out_features=output_shape, # same number of output units as our number of classes\n",
        "                    bias=True)).to(device)"
      ],
      "metadata": {
        "id": "G1-6xV3ZKeSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ],
      "metadata": {
        "id": "XQFaXX8CKePi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_0.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "exxU79eaKeM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# INIT WANDB\n",
        "# -------------------------------\n",
        "# Start a Weights & Biases run to track metrics\n",
        "wandb.init(project=\"transfer-learning-pizza-steak-sushi\")\n",
        "\n",
        "# -------------------------------\n",
        "# TRAINING FUNCTION\n",
        "# -------------------------------\n",
        "def train(model, optimizer, dataloader, loss_fn):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_loss, train_acc = 0, 0  # Initialize accumulators\n",
        "\n",
        "    for X, y in dataloader:\n",
        "        X, y = X.to(device), y.to(device)  # Move data to GPU/CPU\n",
        "        y_pred = model(X)                  # Forward pass\n",
        "        loss = loss_fn(y_pred, y)          # Calculate loss\n",
        "\n",
        "        optimizer.zero_grad()              # Reset gradients\n",
        "        loss.backward()                    # Backpropagation\n",
        "        optimizer.step()                   # Update weights\n",
        "\n",
        "        train_loss += loss.item()          # Add loss\n",
        "        train_acc += accuracy(y_pred.softmax(dim=1), y, task='multiclass', num_classes=3)  # Add accuracy\n",
        "\n",
        "    return train_loss / len(dataloader), train_acc / len(dataloader)\n",
        "\n",
        "# -------------------------------\n",
        "# TESTING FUNCTION\n",
        "# -------------------------------\n",
        "def test(model, dataloader, loss_fn):\n",
        "    model.eval()  # Set model to eval mode\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            test_acc += accuracy(y_pred.softmax(dim=1), y, task='multiclass', num_classes=3)\n",
        "\n",
        "    return test_loss / len(dataloader), test_acc / len(dataloader)\n"
      ],
      "metadata": {
        "id": "KvXLKtBi4XW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "for _ in range(1000):\n",
        "    train(model_0, optimizer, train_dataloader, loss_fn)\n",
        "    test_acc = test(model_0, test_dataloader, loss_fn)\n",
        "    print(f\"Test accuracy: {test_acc}\")  # You should get values around 90% accuracy on the test set\n",
        "\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "ComVkVtuKeKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make a confusion matrix with the test preds and the truth labels"
      ],
      "metadata": {
        "id": "Mb2bQ1b5K2WP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HINT: Look at the torchmetrics.functional.classification.multiclass_confusion_matrix from the torchmetrics library"
      ],
      "metadata": {
        "id": "5I2jpYAcM07s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate predictions for all test data\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "model_0.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X, y in test_dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        preds = model_0(X).argmax(dim=1)  # Convert logits to predicted class\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_labels.append(y.cpu())\n",
        "\n",
        "# Combine all batches into single tensors\n",
        "all_preds = torch.cat(all_preds)\n",
        "all_labels = torch.cat(all_labels)\n",
        "\n",
        "# Compute confusion matrix for 3 classes\n",
        "cm = multiclass_confusion_matrix(preds=all_preds, target=all_labels, num_classes=3)\n",
        "print(\"Confusion Matrix:\", cm)"
      ],
      "metadata": {
        "id": "_5LU9-5Xu7dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Get the \"most wrong\" of the predictions on the test dataset and plot the 5 \"most wrong\" images. You can do this by:\n",
        "* Predicting across all of the test dataset, storing the labels and predicted probabilities.\n",
        "* Sort the predictions by *wrong prediction* and then *descending predicted probabilities*, this will give you the wrong predictions with the *highest* prediction probabilities, in other words, the \"most wrong\".\n",
        "* Plot the top 5 \"most wrong\" images, why do you think the model got these wrong?\n",
        "\n",
        "You'll want to:\n",
        "* Create a DataFrame with sample, label, prediction, pred prob\n",
        "* Sort DataFrame by correct (does label == prediction)\n",
        "* Sort DataFrame by pred prob (descending)\n",
        "* Plot the top 5 \"most wrong\" image predictions"
      ],
      "metadata": {
        "id": "YqlStPo-gbrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create empty list to store information\n",
        "wrong_preds = []\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model_0.eval()\n",
        "\n",
        "# No gradients needed during inference\n",
        "with torch.no_grad():\n",
        "    for X, y in test_dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Get raw predictions (logits)\n",
        "        y_logits = model_0(X)\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        y_probs = torch.softmax(y_logits, dim=1)\n",
        "\n",
        "        # Get predicted class indices and prediction confidences\n",
        "        y_pred_labels = torch.argmax(y_probs, dim=1)\n",
        "        y_pred_probs = torch.max(y_probs, dim=1).values\n",
        "\n",
        "        # Loop through each prediction and store info\n",
        "        for img, true, pred, prob in zip(X.cpu(), y.cpu(), y_pred_labels.cpu(), y_pred_probs.cpu()):\n",
        "            correct = int(true == pred)\n",
        "            wrong_preds.append({\n",
        "                \"image\": img,\n",
        "                \"label\": true.item(),\n",
        "                \"pred\": pred.item(),\n",
        "                \"prob\": prob.item(),\n",
        "                \"correct\": correct\n",
        "            })\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "wrong_df = pd.DataFrame(wrong_preds)\n",
        "\n",
        "# Filter out only the wrong predictions\n",
        "wrong_only = wrong_df[wrong_df[\"correct\"] == 0]\n",
        "\n",
        "# Sort wrong predictions by highest confidence\n",
        "most_wrong = wrong_only.sort_values(by=\"prob\", ascending=False).head(5)"
      ],
      "metadata": {
        "id": "cHtMeYHuvDwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the 5 most wrong predictions\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, row in enumerate(most_wrong.itertuples()):\n",
        "    img = row.image.permute(1, 2, 0).numpy()  # Convert from [C, H, W] to [H, W, C]\n",
        "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Unnormalize\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Label: {class_names[row.label]}\\nPred: {class_names[row.pred]}\\nProb: {row.prob:.2f}\")\n",
        "plt.suptitle(\"Top 5 Most Wrong Predictions\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1hXhd4ajpjlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Train the model from section 4 above with more data, say 20% of the images from Food101 of Pizza, Steak and Sushi images.\n",
        "* You can find the [20% Pizza, Steak, Sushi dataset](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) on the course GitHub. It was created with the notebook [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb).\n"
      ],
      "metadata": {
        "id": "_oRrWPZTgoqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get 20% data"
      ],
      "metadata": {
        "id": "VxyMMnUbgvw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the 20% subset of pizza, steak, sushi from GitHub\n",
        "!wget https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\n",
        "#Create a data directory (if it doesn't already exist)\n",
        "!mkdir -p data\n",
        "#Unzip the dataset into the data/ directory\n",
        "!unzip -n pizza_steak_sushi_20_percent.zip -d data/pizza_steak_sushi_20_percent"
      ],
      "metadata": {
        "id": "3dYjJmDD7mHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create DataLoaders"
      ],
      "metadata": {
        "id": "SQj7eFdSe4Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transforms pipeline\n",
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "])"
      ],
      "metadata": {
        "id": "TEG_k785e7Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoader's as well as get a list of class names\n",
        "\n",
        "data_path = \"data/pizza_steak_sushi\"\n",
        "train_dset_20p= ImageFolder(f\"{data_path}/train\", transform=simple_transform)\n",
        "test_dset_20p = ImageFolder(f\"{data_path}/test\", transform=simple_transform)\n",
        "class_names = list(os.listdir(f\"{data_path}/train\"))  # 'pizza', 'steak', 'sushi'\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "\n",
        "train_dataloader_20p = DataLoader(train_dset_20p, batch_size=32, num_workers=2, shuffle=True)\n",
        "test_dataloader_20p = DataLoader(test_dset_20p, batch_size=32, num_workers=2)\n",
        "\n",
        "# train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "82x7LnQJe7H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get a pretrained model"
      ],
      "metadata": {
        "id": "qROl77sKfIOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "#Load EfficientNetB0 pretrained on ImageNet\n",
        "model_20p = models.efficientnet_b0(pretrained=True).to(device)\n",
        "\n",
        "#Freeze all feature extraction layers\n",
        "for param in model_20p.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#Replace the classifier layer to output 3 classes\n",
        "model_20p.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.2, inplace=True),\n",
        "    nn.Linear(in_features=1280, out_features=len(class_names))  # 3 output classes\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "PHWNZ6yDvpR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a model with 20% of the data"
      ],
      "metadata": {
        "id": "UqffJfOIfp3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "#Define loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "#Define optimizer (trainable parameters only)\n",
        "optimizer = torch.optim.Adam(model_20p.parameters(), lr=1e-3)\n",
        "# Initialize wandb run\n",
        "wandb.init(project=\"transfer-learning-pizza-steak-sushi\", name=\"efficientnet-20percent\")\n",
        "\n",
        "\n",
        "\n",
        "# ✅ Training function\n",
        "def train(model, dataloader, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    total_loss, total_acc = 0, 0\n",
        "    for X, y in dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        preds = model(X)\n",
        "        loss = loss_fn(preds, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_acc += (preds.argmax(dim=1) == y).sum().item()\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader.dataset)\n",
        "\n",
        "# ✅ Testing function\n",
        "def test(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss, total_acc = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, y)\n",
        "            total_loss += loss.item()\n",
        "            total_acc += (preds.argmax(dim=1) == y).sum().item()\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "# ✅ Train for 5 epochs\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train(model_20p, train_dataloader_20p, loss_fn, optimizer)\n",
        "    test_loss, test_acc = test(model_20p, test_dataloader_20p, loss_fn)\n",
        "\n",
        "    # ✅ Log results to wandb\n",
        "    wandb.log({\n",
        "        \"train_loss_20p\": train_loss,\n",
        "        \"train_acc_20p\": train_acc,\n",
        "        \"test_loss_20p\": test_loss,\n",
        "        \"test_acc_20p\": test_acc\n",
        "    })\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "wXpYOYeTvp7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Try a different model from [`torchvision.models`](https://pytorch.org/vision/stable/models.html) on the Pizza, Steak, Sushi data, how does this model perform?\n",
        "* You'll have to change the size of the classifier layer to suit our problem.\n",
        "* You may want to try an EfficientNet with a higher number than our B0, perhaps `torchvision.models.efficientnet_b2()`?\n",
        "  * **Note:** Depending on the model you use you will have to prepare/transform the data in a certain way."
      ],
      "metadata": {
        "id": "Ibj4UPjRgvly"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3FQ8tL7El7eO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}